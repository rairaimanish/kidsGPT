from flask import Flask, request, jsonify, send_file
from flask_cors import CORS
import torch
import torchaudio
import whisper
from mlx_lm import load, generate
import ChatTTS
import os

app = Flask(__name__)
CORS(app)
# Load the STT model
stt_model = whisper.load_model("base")

# Load the LLM
model, tokenizer = load('Qwen/Qwen2-7B-Instruct-MLX', tokenizer_config={"eos_token": "<|im_end|>"})

# Load the TTS model
chat = ChatTTS.Chat()
chat.load(compile=True)

# Step 1: Transcribe the audio and return transcription
@app.route('/api/transcribe', methods=['POST'])
def transcribe_audio():
    try:
        audio_file = request.files.get('audio')
        if not audio_file:
            return jsonify({"error": "No audio file provided"}), 400

        audio_path = "prompt.wav"
        audio_file.save(audio_path)

        # Step 1: Transcribe audio using Whisper (STT)
        result = stt_model.transcribe(audio_path)
        transcription = result["text"]

        return jsonify({"transcription": transcription})
    except Exception as e:
        return jsonify({"error": str(e)}), 500

# Step 2: Generate LLM text response first
@app.route('/api/respond', methods=['POST'])
def respond_with_text():
    try:
        transcription = request.json.get("transcription")
        if not transcription:
            return jsonify({"error": "No transcription provided"}), 400

        # Generate response using LLM
        messages = [
            {"role": "system", "content": "You are a children expert. Respond appropriately. Keep your answer simple and concise as if speaking to a child. Avoid using any special characters."},
            {"role": "user", "content": transcription}
        ]
        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

        response = generate(model, tokenizer, prompt=prompt, verbose=True, top_p=0.8, temp=0.7, repetition_penalty=1.05, max_tokens=512)
        generated_text = response

        # Return text response only for now
        return jsonify({
            "response_text": generated_text
        })
    except Exception as e:
        return jsonify({"error": str(e)}), 500

# Step 3: Generate TTS audio from the text (to be requested later)
@app.route('/api/generate_audio', methods=['POST'])
def generate_audio():
    try:
        response_text = request.json.get("response_text")
        if not response_text:
            return jsonify({"error": "No response text provided"}), 400

        # Convert text to speech using ChatTTS
        texts = [response_text]
        wavs = chat.infer(texts)

        # Save the audio response to a file
        output_wav_path = "response.wav"
        torchaudio.save(output_wav_path, torch.from_numpy(wavs[0]), 24000)

        return jsonify({
            "audio_url": "/api/audio"
        })
    except Exception as e:
        return jsonify({"error": str(e)}), 500

# Route to serve the generated audio file
@app.route('/api/audio', methods=['GET'])
def get_audio():
    try:
        return send_file("response.wav", as_attachment=False, mimetype="audio/wav")
    except Exception as e:
        return jsonify({"error": str(e)}), 500

# Step 4: Respond directly to text input (bypassing transcription)
@app.route('/api/respond_text', methods=['POST'])
def respond_with_text_input():
    try:
        user_input = request.json.get("text")
        if not user_input:
            return jsonify({"error": "No text input provided"}), 400

        # Generate response using LLM
        messages = [
            {"role": "system", "content": "You are a children expert. Respond appropriately. Keep your answer simple and concise as if speaking to a child. Avoid using any special characters."},
            {"role": "user", "content": user_input}
        ]
        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

        response = generate(model, tokenizer, prompt=prompt, verbose=True, top_p=0.8, temp=0.7, repetition_penalty=1.05, max_tokens=512)
        generated_text = response

        return jsonify({
            "response_text": generated_text
        })
    except Exception as e:
        return jsonify({"error": str(e)}), 500

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
