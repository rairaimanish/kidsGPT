from flask import Flask, request, jsonify, send_file
from flask_cors import CORS
import torch
import torchaudio
import whisper
from transformers import AutoModelForCausalLM, AutoTokenizer
import ChatTTS
import os

app = Flask(__name__)
CORS(app)

# Load the STT model
stt_model = whisper.load_model("base")

# Load the LLM model and tokenizer using transformers
model_name = "Qwen/Qwen2.5-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Load the TTS model
chat = ChatTTS.Chat()
chat.load(compile=True)

def generate_llm_response(transcription):
    # Prepare the messages for the LLM
    messages = [
        {"role": "system", "content": "You are a children expert. Respond as if you are talking to a child. Avoid using any special character."},
        {"role": "user", "content": transcription},
    ]
    # Create the input prompt using the tokenizer's template
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
    )
    # Tokenize the input text for the model
    model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
    
    # Generate the response using the LLM
    generated_ids = model.generate(
        **model_inputs,
        max_new_tokens=512,
    )
    # Extract the generated tokens, skipping the input tokens
    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    ]
    # Decode the generated text
    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    return response

# Step 1: Transcribe the audio and return transcription
@app.route('/api/transcribe', methods=['POST'])
def transcribe_audio():
    try:
        audio_file = request.files.get('audio')
        if not audio_file:
            return jsonify({"error": "No audio file provided"}), 400

        audio_path = "prompt.wav"
        audio_file.save(audio_path)

        # Step 1: Transcribe audio using Whisper (STT)
        result = stt_model.transcribe(audio_path)
        transcription = result["text"]

        return jsonify({"transcription": transcription})
    except Exception as e:
        return jsonify({"error": str(e)}), 500

# Step 2: Generate LLM text response first
@app.route('/api/respond', methods=['POST'])
def respond_with_text():
    try:
        transcription = request.json.get("transcription")
        if not transcription:
            return jsonify({"error": "No transcription provided"}), 400

        # Generate response using LLM with the updated function
        generated_text = generate_llm_response(transcription)

        # Return text response only for now
        return jsonify({
            "response_text": generated_text
        })
    except Exception as e:
        return jsonify({"error": str(e)}), 500

# Step 3: Generate TTS audio from the text (to be requested later)
@app.route('/api/generate_audio', methods=['POST'])
def generate_audio():
    try:
        response_text = request.json.get("response_text")
        if not response_text:
            return jsonify({"error": "No response text provided"}), 400

        # Convert text to speech using ChatTTS
        texts = [response_text]
        wavs = chat.infer(texts)

        # Save the audio response to a file
        output_wav_path = "response.wav"
        torchaudio.save(output_wav_path, torch.from_numpy(wavs[0]), 24000)

        return jsonify({
            "audio_url": "/api/audio"
        })
    except Exception as e:
        return jsonify({"error": str(e)}), 500

# Route to serve the generated audio file
@app.route('/api/audio', methods=['GET'])
def get_audio():
    try:
        return send_file("response.wav", as_attachment=False, mimetype="audio/wav")
    except Exception as e:
        return jsonify({"error": str(e)}), 500

# Step 4: Respond directly to text input (bypassing transcription)
@app.route('/api/respond_text', methods=['POST'])
def respond_with_text_input():
    try:
        user_input = request.json.get("text")
        if not user_input:
            return jsonify({"error": "No text input provided"}), 400

        # Generate response using LLM with the updated function
        generated_text = generate_llm_response(user_input)

        return jsonify({
            "response_text": generated_text
        })
    except Exception as e:
        return jsonify({"error": str(e)}), 500

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
