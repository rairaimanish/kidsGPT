from flask import Flask, request, jsonify, session
import torch
import torchaudio
import whisper
from mlx_lm import load, generate
import ChatTTS
import os

app = Flask(__name__)
app.secret_key = "your_secret_key"  # For session management

# Load the STT model
stt_model = whisper.load_model("base")

# Load the LLM
model, tokenizer = load('Qwen/Qwen2-7B-Instruct-MLX', tokenizer_config={"eos_token": "<|im_end|>"})

# Load the TTS model
chat = ChatTTS.Chat()
chat.load(compile=True)

# In-memory conversation history store (can be replaced with a database)
conversation_histories = {}

MAX_MEMORY_SIZE = 5  # Maximum number of messages to remember

def get_conversation_history(session_id):
    """Retrieve the conversation history for a specific session ID."""
    if session_id not in conversation_histories:
        conversation_histories[session_id] = []
    return conversation_histories[session_id]

def update_conversation_history(session_id, role, content):
    """Update the conversation history with a new message, limited to MAX_MEMORY_SIZE."""
    conversation_history = get_conversation_history(session_id)
    conversation_history.append({"role": role, "content": content})
    
    # Limit the conversation history to the last MAX_MEMORY_SIZE messages
    if len(conversation_history) > MAX_MEMORY_SIZE:
        conversation_history = conversation_history[-MAX_MEMORY_SIZE:]
    
    conversation_histories[session_id] = conversation_history

# Step 1: Transcribe the audio and return transcription
@app.route('/api/transcribe', methods=['POST'])
def transcribe_audio():
    try:
        audio_file = request.files.get('audio')
        if not audio_file:
            return jsonify({"error": "No audio file provided"}), 400

        audio_path = "prompt.wav"
        audio_file.save(audio_path)

        # Step 1: Transcribe audio using Whisper (STT)
        result = stt_model.transcribe(audio_path)
        transcription = result["text"]

        return jsonify({"transcription": transcription})
    except Exception as e:
        return jsonify({"error": str(e)}), 500

# Step 2: Generate LLM response + convert to speech (TTS)
@app.route('/api/respond', methods=['POST'])
def respond_with_audio():
    try:
        transcription = request.json.get("transcription")
        if not transcription:
            return jsonify({"error": "No transcription provided"}), 400

        # Get session ID for the user
        session_id = session.get("session_id", str(len(conversation_histories)))

        # Update the conversation history with the user's transcription
        update_conversation_history(session_id, "user", transcription)

        # Get the full conversation history to pass it to the LLM
        conversation_history = get_conversation_history(session_id)

        # Step 2: Generate response using LLM with memory
        prompt = tokenizer.apply_chat_template(conversation_history, tokenize=False, add_generation_prompt=True)

        response = generate(model, tokenizer, prompt=prompt, verbose=True, top_p=0.8, temp=0.7, repetition_penalty=1.05, max_tokens=512)
        generated_text = response

        # Update the conversation history with the bot's response
        update_conversation_history(session_id, "bot", generated_text)

        # Return the text response for now (audio generation will be handled separately)
        return jsonify({
            "response_text": generated_text
        })
    except Exception as e:
        return jsonify({"error": str(e)}), 500

# Step 3: Generate TTS audio from the text (to be requested later)
@app.route('/api/generate_audio', methods=['POST'])
def generate_audio():
    try:
        response_text = request.json.get("response_text")
        if not response_text:
            return jsonify({"error": "No response text provided"}), 400

        # Convert text to speech using ChatTTS
        texts = [response_text]
        wavs = chat.infer(texts)

        # Save the audio response to a file
        output_wav_path = "response.wav"
        torchaudio.save(output_wav_path, torch.from_numpy(wavs[0]), 24000)

        return jsonify({
            "audio_url": "/api/audio"
        })
    except Exception as e:
        return jsonify({"error": str(e)}), 500

# Route to serve the generated audio file
@app.route('/api/audio', methods=['GET'])
def get_audio():
    try:
        return send_file("response.wav", as_attachment=False, mimetype="audio/wav")
    except Exception as e:
        return jsonify({"error": str(e)}), 500

# Step 4: Respond directly to text input (bypassing transcription)
@app.route('/api/respond_text', methods=['POST'])
def respond_with_text_input():
    try:
        user_input = request.json.get("text")
        if not user_input:
            return jsonify({"error": "No text input provided"}), 400

        # Get session ID for the user
        session_id = session.get("session_id", str(len(conversation_histories)))

        # Update the conversation history with the user's input
        update_conversation_history(session_id, "user", user_input)

        # Get the full conversation history to pass it to the LLM
        conversation_history = get_conversation_history(session_id)

        # Step 1: Generate response using LLM with memory
        prompt = tokenizer.apply_chat_template(conversation_history, tokenize=False, add_generation_prompt=True)

        response = generate(model, tokenizer, prompt=prompt, verbose=True, top_p=0.8, temp=0.7, repetition_penalty=1.05, max_tokens=512)
        generated_text = response

        # Update the conversation history with the bot's response
        update_conversation_history(session_id, "bot", generated_text)

        return jsonify({
            "response_text": generated_text
        })
    except Exception as e:
        return jsonify({"error": str(e)}), 500

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
